{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11134917,"sourceType":"datasetVersion","datasetId":6944884}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n# Install and upgrade Unsloth (and its latest version)\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:26:15.582735Z","iopub.execute_input":"2025-03-24T02:26:15.583060Z","iopub.status.idle":"2025-03-24T02:29:25.235086Z","shell.execute_reply.started":"2025-03-24T02:26:15.583034Z","shell.execute_reply":"2025-03-24T02:29:25.234147Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:29:25.236417Z","iopub.execute_input":"2025-03-24T02:29:25.236666Z","iopub.status.idle":"2025-03-24T02:29:55.293262Z","shell.execute_reply.started":"2025-03-24T02:29:25.236642Z","shell.execute_reply":"2025-03-24T02:29:55.292372Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\nü¶• Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"HF_TOKEN\")\nwnb_token = user_secrets.get_secret(\"wnb\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb\nrun = wandb.init(\n    project='Fine-tune-Gemma 7B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:29:55.295136Z","iopub.execute_input":"2025-03-24T02:29:55.295436Z","iopub.status.idle":"2025-03-24T02:30:08.624781Z","shell.execute_reply.started":"2025-03-24T02:29:55.295413Z","shell.execute_reply":"2025-03-24T02:30:08.624175Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarthik-nimmagadda\u001b[0m (\u001b[33mkarthik-nimmagadda-san-jose-state-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250324_023002-p2d29mlk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset/runs/p2d29mlk' target=\"_blank\">grateful-forest-5</a></strong> to <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset/runs/p2d29mlk' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset/runs/p2d29mlk</a>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default \nload_in_4bit = True # Enables 4 bit quantization ‚Äî a memory saving optimization \n\n# Load the DeepSeek R1 model and tokenizer using unsloth ‚Äî imported using: from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/gemma-7b\",  # Load the pre-trained gemma model (7B parameter version)\n    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n    token=hugging_face_token, # Use hugging face token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:30:08.625899Z","iopub.execute_input":"2025-03-24T02:30:08.626176Z","iopub.status.idle":"2025-03-24T02:30:40.354247Z","shell.execute_reply.started":"2025-03-24T02:30:08.626154Z","shell.execute_reply":"2025-03-24T02:30:40.353264Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.18: Fast Gemma patching. Transformers: 4.50.0.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.57G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7eae7132ba34eb984867cfadf243fc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0aea236b23c4f1ebeb204f560aa729b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"adcd8a9690cd40faafbcc8dfac39c840"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e029f0febe324528967f144d094d9b04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e19bf49ff6004b169f76bd6b9452cb06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e6730e4693f4b11b6b19cab45734347"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Define prompt styles\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:30:40.355214Z","iopub.execute_input":"2025-03-24T02:30:40.355521Z","iopub.status.idle":"2025-03-24T02:30:40.360170Z","shell.execute_reply.started":"2025-03-24T02:30:40.355490Z","shell.execute_reply":"2025-03-24T02:30:40.359309Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Training prompt style updated to include both <think> and </think> tags:\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:30:40.360978Z","iopub.execute_input":"2025-03-24T02:30:40.361222Z","iopub.status.idle":"2025-03-24T02:30:45.381043Z","shell.execute_reply.started":"2025-03-24T02:30:40.361203Z","shell.execute_reply":"2025-03-24T02:30:45.380303Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load the dataset from Kaggle (assumes the dataset has fields: \"context\", \"input\", \"output\")\ndataset = load_dataset(\"/kaggle/input/my-datasets/\", split=\"train\", trust_remote_code=True)\n\n# Split into training (80%), and temp (20%)\nsplit_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset[\"train\"]\ntemp_dataset = split_dataset[\"test\"]\n\n# Split the temporary dataset equally into evaluation (10%) and test (10%)\ntemp_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\neval_dataset = temp_split[\"train\"]\ntest_dataset = temp_split[\"test\"]\n\nprint(\"Training samples:\", len(train_dataset))\nprint(\"Evaluation samples:\", len(eval_dataset))\nprint(\"Test samples:\", len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:30:45.382049Z","iopub.execute_input":"2025-03-24T02:30:45.382314Z","iopub.status.idle":"2025-03-24T02:30:45.889773Z","shell.execute_reply.started":"2025-03-24T02:30:45.382292Z","shell.execute_reply":"2025-03-24T02:30:45.889018Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86caebbf677745dfb9cb63e4a1b6d581"}},"metadata":{}},{"name":"stdout","text":"Training samples: 11330\nEvaluation samples: 1416\nTest samples: 1417\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# 2. Format the Datasets for Fine-tuning\n# ------------------------------\nEOS_TOKEN = tokenizer.eos_token  # End-of-sequence token\n\ndef formatting_prompts_func(examples):\n    inputs_list = examples[\"context\"]  # Medical question\n    cots_list = examples[\"input\"]      # Chain-of-thought reasoning\n    outputs_list = examples[\"output\"]  # Final answer\n    texts = []\n    for inp, cot, out in zip(inputs_list, cots_list, outputs_list):\n        text = train_prompt_style.format(inp, cot, out) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Map the formatting function over all splits\ntrain_dataset_formatted = train_dataset.map(formatting_prompts_func, batched=True)\neval_dataset_formatted  = eval_dataset.map(formatting_prompts_func, batched=True)\ntest_dataset_formatted  = test_dataset.map(formatting_prompts_func, batched=True)\n\n# (Optional) Inspect one formatted example\nprint(\"Formatted prompt example:\\n\", train_dataset_formatted[\"text\"][6])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:30:45.890579Z","iopub.execute_input":"2025-03-24T02:30:45.890787Z","iopub.status.idle":"2025-03-24T02:30:46.281738Z","shell.execute_reply.started":"2025-03-24T02:30:45.890768Z","shell.execute_reply":"2025-03-24T02:30:46.280815Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11330 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c8a8fa7bb184301b0f8422ebea23f50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1416 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee4e39c2caaa4e60890fdd547a751ef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1417 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dcccfe70bfd4a40933899ebd0887114"}},"metadata":{}},{"name":"stdout","text":"Formatted prompt example:\n Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\nWhat causes pain in right upper jaw and headache?\n\n### Response:\n<think>\nI have a blister filled bump on my upper front gum line I have been taking antibiotics and 2 days ago the bump popped and it drained yellow puss out  and then blood my tooth is broken off in the gum line the tooth does not hurt im curious is to what is going on\n</think>\nThanks for your query, I have gone through your query.The blister bump on the gums can be a pus discharging sinus tract secondary to an infected tooth or a fractured tooth. Nothing to be panic, complete the course of antibiotics particularly a combination of amoxicillin and metronidazole. Then get a radiograph done to check the amount of tooth structure remaining and surrounding bone level. If sufficient bone and tooth structure is there, then it can be restored with root canal treatment otherwise, it has to be removed.I hope my answer will help you, take care.<eos>\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 3. Prepare for Fine-tuning with LoRA\n# ------------------------------\n# Apply LoRA to the model\nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# Monkey-patch for safe training (restores model.generate if needed)\ndef safe_for_training(model, use_gradient_checkpointing=True):\n    if hasattr(model, \"_unwrapped_old_generate\"):\n        model.generate = model._unwrapped_old_generate\n        try:\n            del model._unwrapped_old_generate\n        except AttributeError:\n            pass\n    return model\n\nmodel_lora.for_training = lambda: safe_for_training(model_lora)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:30:46.284129Z","iopub.execute_input":"2025-03-24T02:30:46.284380Z","iopub.status.idle":"2025-03-24T02:30:52.802634Z","shell.execute_reply.started":"2025-03-24T02:30:46.284360Z","shell.execute_reply":"2025-03-24T02:30:52.801672Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.18 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 4. Initialize the Fine-tuning Trainer\n# ------------------------------\ntrainer = SFTTrainer(\n    model=model_lora,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset_formatted,\n    eval_dataset=eval_dataset_formatted,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,\n        warmup_steps=5,\n        max_steps=30,\n        evaluation_strategy=\"steps\",  # Enable periodic evaluation\n        eval_steps=10,                # Evaluate every 10 steps\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:30:52.803696Z","iopub.execute_input":"2025-03-24T02:30:52.803938Z","iopub.status.idle":"2025-03-24T02:31:06.134756Z","shell.execute_reply.started":"2025-03-24T02:30:52.803917Z","shell.execute_reply":"2025-03-24T02:31:06.134043Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/11330 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b69f863f984428bef798dd1670f4d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/1416 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9f1a8ee3e58484ba75ca37102f71db3"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# 5. Fine-tune the Model\n# ------------------------------\ntrainer_stats = trainer.train()\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T02:31:06.135857Z","iopub.execute_input":"2025-03-24T02:31:06.136191Z","iopub.status.idle":"2025-03-24T04:05:07.158743Z","shell.execute_reply.started":"2025-03-24T02:31:06.136145Z","shell.execute_reply":"2025-03-24T04:05:07.158105Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 11,330 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 50,003,968/7,000,000,000 (0.71% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 1:33:26, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.430300</td>\n      <td>1.865368</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.916200</td>\n      <td>1.819648</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.845900</td>\n      <td>1.788210</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but GemmaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñá‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.78821</td></tr><tr><td>eval/runtime</td><td>1600.9221</td></tr><tr><td>eval/samples_per_second</td><td>0.884</td></tr><tr><td>eval/steps_per_second</td><td>0.442</td></tr><tr><td>total_flos</td><td>4548567289872384.0</td></tr><tr><td>train/epoch</td><td>0.02118</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>0.93097</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.8459</td></tr><tr><td>train_loss</td><td>2.06412</td></tr><tr><td>train_runtime</td><td>5637.2245</td></tr><tr><td>train_samples_per_second</td><td>0.043</td></tr><tr><td>train_steps_per_second</td><td>0.005</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">grateful-forest-5</strong> at: <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset/runs/p2d29mlk' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset/runs/p2d29mlk</a><br> View project at: <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune-Gemma%207B%20on%20Medical%20Dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250324_023002-p2d29mlk/logs</code>"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Call for_inference to enable Unsloth‚Äôs patched generation\nFastLanguageModel.for_inference(model)\n\n# -- OPTIONAL MONKEY-PATCH --\n# If you keep getting \"must call for_inference\" or need to bypass fast inference:\ndef generate_no_fast_inference(*args, **kwargs):\n    return model_lora.base_model.generate(*args, **kwargs)\n\n# Comment out the next line if you want to preserve Unsloth's speedups:\nmodel_lora.generate = generate_no_fast_inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:05:07.159547Z","iopub.execute_input":"2025-03-24T04:05:07.159828Z","iopub.status.idle":"2025-03-24T04:05:07.164031Z","shell.execute_reply.started":"2025-03-24T04:05:07.159804Z","shell.execute_reply":"2025-03-24T04:05:07.163234Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"question = \"\"\"I have osteoporosis mainly in the spine.I exercise 5 days a werk to help the situation. I don t know if there are any drugs that really help reverse the situation. What is your feeling about Prolia and its sidr effects. I have read them all. Also anything else you can suggest. I am in my 70s.\"\"\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=160,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:07:40.587394Z","iopub.execute_input":"2025-03-24T04:07:40.587697Z","iopub.status.idle":"2025-03-24T04:07:49.754738Z","shell.execute_reply.started":"2025-03-24T04:07:40.587676Z","shell.execute_reply":"2025-03-24T04:07:49.753949Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nI have osteoporosis mainly in the spine. I exercise 5 days a week to help the situation. I don t know if there are any drugs that really help reverse the situation. What is your feeling about Prolia and its side effects. I have read them all. Also anything else you can suggest. I am in my 70s.\n</think>\nHi, I am a 70 year old female. I have osteoporosis mainly in the spine. I exercise 5 days a week to help the situation. I don t know if there are any drugs that really help reverse the situation. What is your feeling about Prolia and its side effects. I have read them all. Also anything else you can suggest. I am in my 70s. Thank you\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"question = \"\"\"I was hit on the forehead and went flying off a bed and hit the back of my head I have two indents in a line from my hairline to my eye, they are about 2 cm deep and a finger wide, they are about 5 cm apart, there isnt any bruising but its tender and I have a sharp pain there I had memory loss for about 12 hours, blurred vision, headache, nausea, dizziness, a bump on the back of my head and neck pain all the way down into my back I was wondering if I should get it checked, im avoiding it because I dont want anyone mad at the person because it was an accident, so I dont want to go unless necessary\"\"\"\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=320,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:07:10.168843Z","iopub.execute_input":"2025-03-24T04:07:10.169171Z","iopub.status.idle":"2025-03-24T04:07:25.505732Z","shell.execute_reply.started":"2025-03-24T04:07:10.169128Z","shell.execute_reply":"2025-03-24T04:07:25.504920Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nI was hit on the forehead and went flying off a bed and hit the back of my head I have two indents in a line from my hairline to my eye, they are about 2 cm deep and a finger wide, they are about 5 cm apart, there isnt any bruising but its tender and I have a sharp pain there I had memory loss for about 12 hours, blurred vision, headache, nausea, dizziness, a bump on the back of my head and neck pain all the way down into my back I was wondering if I should get it checked, im avoiding it because I dont want anyone mad at the person because it was an accident, so I dont want to go unless necessary\n</think>\nHi, I have gone through your query and here is my advice. I am sorry to hear about your accident. I understand your concern. I would like to know if you have any other symptoms like vomiting, loss of consciousness, confusion, seizures, slurred speech, difficulty in walking, etc. If you have any of these symptoms, then you should get an MRI brain done. If you do not have any of these symptoms, then you do not need to get an MRI brain done. I hope this helps. If you have any other questions, I will be happy to help. Take care.<eos>\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 7. Deploy the Fine-tuned Model on Hugging Face\n# ------------------------------\n# (Make sure you are logged in using your Hugging Face token)\nmodel_repo = \"KarthikNimmagadda/Gemma-Finetuned-Medical-Dataset\" # Update with your Hugging Face repository name\nmodel_lora.push_to_hub(model_repo, use_auth_token=hugging_face_token)\ntokenizer.push_to_hub(model_repo, use_auth_token=hugging_face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T04:05:52.480498Z","iopub.execute_input":"2025-03-24T04:05:52.480817Z","iopub.status.idle":"2025-03-24T04:06:08.051724Z","shell.execute_reply.started":"2025-03-24T04:05:52.480790Z","shell.execute_reply":"2025-03-24T04:06:08.050789Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/580 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e04d5f27b25346f2bcb33914e68efab3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e3f028ec0bd41619f779d285520d3ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/200M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"174fd1825e404578a99b751911a4a1be"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/KarthikNimmagadda/Gemma-Finetuned-Medical-Dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c7f76baa36d44f59a4d9172f0db3e08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/34.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae63acd9d4e44a408e51e661fb9abe9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80d6135c0ae442709bd27085a106f84b"}},"metadata":{}}],"execution_count":15}]}