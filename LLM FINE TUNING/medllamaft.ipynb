{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11134917,"sourceType":"datasetVersion","datasetId":6944884}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall torch -y\n!pip install torch --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:27:52.381796Z","iopub.execute_input":"2025-03-30T14:27:52.382070Z","iopub.status.idle":"2025-03-30T14:30:29.325246Z","shell.execute_reply.started":"2025-03-30T14:27:52.382041Z","shell.execute_reply":"2025-03-30T14:30:29.324368Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.5.1+cu121\nUninstalling torch-2.5.1+cu121:\n  Successfully uninstalled torch-2.5.1+cu121\nCollecting torch\n  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting nvidia-nccl-cu12==2.21.5 (from torch)\n  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting triton==3.2.0 (from torch)\n  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nDownloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-cusparselt-cu12, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\ntorchvision 0.20.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 torch-2.6.0 triton-3.2.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n# Install and upgrade Unsloth (and its latest version)\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:30:29.326161Z","iopub.execute_input":"2025-03-30T14:30:29.326490Z","iopub.status.idle":"2025-03-30T14:31:10.870383Z","shell.execute_reply.started":"2025-03-30T14:30:29.326446Z","shell.execute_reply":"2025-03-30T14:31:10.869479Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:31:10.871288Z","iopub.execute_input":"2025-03-30T14:31:10.871580Z","iopub.status.idle":"2025-03-30T14:31:41.370612Z","shell.execute_reply.started":"2025-03-30T14:31:10.871557Z","shell.execute_reply":"2025-03-30T14:31:41.369725Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"HF_TOKEN\")\nwnb_token = user_secrets.get_secret(\"wnb\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb\nrun = wandb.init(\n    project='Fine-tune Medllama 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:31:41.371546Z","iopub.execute_input":"2025-03-30T14:31:41.371840Z","iopub.status.idle":"2025-03-30T14:31:56.863388Z","shell.execute_reply.started":"2025-03-30T14:31:41.371810Z","shell.execute_reply":"2025-03-30T14:31:56.862711Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarthik-nimmagadda\u001b[0m (\u001b[33mkarthik-nimmagadda-san-jose-state-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250330_143149-tkwpf75p</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset/runs/tkwpf75p' target=\"_blank\">kind-darkness-7</a></strong> to <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset/runs/tkwpf75p' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset/runs/tkwpf75p</a>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default \nload_in_4bit = True # Enables 4 bit quantization â€” a memory saving optimization \n\n# Load the DeepSeek R1 model and tokenizer using unsloth â€” imported using: from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"johnsnowlabs/JSL-MedLlama-3-8B-v2.0\",  # Load the pre-trained gemma model (7B parameter version)\n    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n    token=hugging_face_token, # Use hugging face token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:31:56.864229Z","iopub.execute_input":"2025-03-30T14:31:56.864497Z","iopub.status.idle":"2025-03-30T14:33:58.560579Z","shell.execute_reply.started":"2025-03-30T14:31:56.864469Z","shell.execute_reply":"2025-03-30T14:33:58.559804Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/22.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"162f6810cc9c46a08a240d6000be46ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0096bb85d81044c9957bf0eb05e5e003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/6.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff41376080644d0a5f4175dae1d85b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39df3998e1f243e8985643582eaa885f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d4a0965eddd4430ba79723e4d9ef1ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd65bd0fd2824af3b599a29d49cb100a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/449 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15b1c081681044d68f672efc76d406ae"}},"metadata":{}},{"name":"stdout","text":"johnsnowlabs/JSL-MedLlama-3-8B-v2.0 does not have a padding token! Will use pad_token = <|reserved_special_token_250|>.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Define prompt styles\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:33:58.562870Z","iopub.execute_input":"2025-03-30T14:33:58.563087Z","iopub.status.idle":"2025-03-30T14:33:58.567169Z","shell.execute_reply.started":"2025-03-30T14:33:58.563069Z","shell.execute_reply":"2025-03-30T14:33:58.566458Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Training prompt style updated to include both <think> and </think> tags:\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:33:58.568505Z","iopub.execute_input":"2025-03-30T14:33:58.568786Z","iopub.status.idle":"2025-03-30T14:33:58.583586Z","shell.execute_reply.started":"2025-03-30T14:33:58.568765Z","shell.execute_reply":"2025-03-30T14:33:58.582805Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load the dataset from Kaggle (assumes the dataset has fields: \"context\", \"input\", \"output\")\ndataset = load_dataset(\"/kaggle/input/my-datasets/\", split=\"train\", trust_remote_code=True)\n\n# Split into training (80%), and temp (20%)\nsplit_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset[\"train\"]\ntemp_dataset = split_dataset[\"test\"]\n\n# Split the temporary dataset equally into evaluation (10%) and test (10%)\ntemp_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\neval_dataset = temp_split[\"train\"]\ntest_dataset = temp_split[\"test\"]\n\nprint(\"Training samples:\", len(train_dataset))\nprint(\"Evaluation samples:\", len(eval_dataset))\nprint(\"Test samples:\", len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:33:58.584338Z","iopub.execute_input":"2025-03-30T14:33:58.584648Z","iopub.status.idle":"2025-03-30T14:33:59.157223Z","shell.execute_reply.started":"2025-03-30T14:33:58.584618Z","shell.execute_reply":"2025-03-30T14:33:59.156364Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8eab28242084586a9e497c862592d2d"}},"metadata":{}},{"name":"stdout","text":"Training samples: 11330\nEvaluation samples: 1416\nTest samples: 1417\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 2. Format the Datasets for Fine-tuning\n# ------------------------------\nEOS_TOKEN = tokenizer.eos_token  # End-of-sequence token\n\ndef formatting_prompts_func(examples):\n    inputs_list = examples[\"context\"]  # Medical question\n    cots_list = examples[\"input\"]      # Chain-of-thought reasoning\n    outputs_list = examples[\"output\"]  # Final answer\n    texts = []\n    for inp, cot, out in zip(inputs_list, cots_list, outputs_list):\n        text = train_prompt_style.format(inp, cot, out) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Map the formatting function over all splits\ntrain_dataset_formatted = train_dataset.map(formatting_prompts_func, batched=True)\neval_dataset_formatted  = eval_dataset.map(formatting_prompts_func, batched=True)\ntest_dataset_formatted  = test_dataset.map(formatting_prompts_func, batched=True)\n\n# (Optional) Inspect one formatted example\nprint(\"Formatted prompt example:\\n\", train_dataset_formatted[\"text\"][6])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:33:59.158296Z","iopub.execute_input":"2025-03-30T14:33:59.158653Z","iopub.status.idle":"2025-03-30T14:33:59.557246Z","shell.execute_reply.started":"2025-03-30T14:33:59.158615Z","shell.execute_reply":"2025-03-30T14:33:59.556540Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11330 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33e86be3addf4f229558fcb7f9d8d026"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1416 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1756b57a026d4d63a258bf24c3c0bb57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1417 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dd90f4e005b467db6cb55a47e157ff9"}},"metadata":{}},{"name":"stdout","text":"Formatted prompt example:\n Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\nWhat causes pain in right upper jaw and headache?\n\n### Response:\n<think>\nI have a blister filled bump on my upper front gum line I have been taking antibiotics and 2 days ago the bump popped and it drained yellow puss out  and then blood my tooth is broken off in the gum line the tooth does not hurt im curious is to what is going on\n</think>\nThanks for your query, I have gone through your query.The blister bump on the gums can be a pus discharging sinus tract secondary to an infected tooth or a fractured tooth. Nothing to be panic, complete the course of antibiotics particularly a combination of amoxicillin and metronidazole. Then get a radiograph done to check the amount of tooth structure remaining and surrounding bone level. If sufficient bone and tooth structure is there, then it can be restored with root canal treatment otherwise, it has to be removed.I hope my answer will help you, take care.<|end_of_text|>\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 3. Prepare for Fine-tuning with LoRA\n# ------------------------------\n# Apply LoRA to the model\nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# Monkey-patch for safe training (restores model.generate if needed)\ndef safe_for_training(model, use_gradient_checkpointing=True):\n    if hasattr(model, \"_unwrapped_old_generate\"):\n        model.generate = model._unwrapped_old_generate\n        try:\n            del model._unwrapped_old_generate\n        except AttributeError:\n            pass\n    return model\n\nmodel_lora.for_training = lambda: safe_for_training(model_lora)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:33:59.558079Z","iopub.execute_input":"2025-03-30T14:33:59.558361Z","iopub.status.idle":"2025-03-30T14:34:06.399225Z","shell.execute_reply.started":"2025-03-30T14:33:59.558327Z","shell.execute_reply":"2025-03-30T14:34:06.398333Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 4. Initialize the Fine-tuning Trainer\n# ------------------------------\ntrainer = SFTTrainer(\n    model=model_lora,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset_formatted,\n    eval_dataset=eval_dataset_formatted,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,\n        warmup_steps=5,\n        max_steps=60,\n        evaluation_strategy=\"steps\",  # Enable periodic evaluation\n        eval_steps=10,                # Evaluate every 10 steps\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:34:06.400032Z","iopub.execute_input":"2025-03-30T14:34:06.400265Z","iopub.status.idle":"2025-03-30T14:34:17.426450Z","shell.execute_reply.started":"2025-03-30T14:34:06.400245Z","shell.execute_reply":"2025-03-30T14:34:17.425493Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/11330 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47ff2dfdf5e14a8cba22fed7bf78bf28"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/1416 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c35b750d7b0464bbb4811d46f79cbf3"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# 5. Fine-tune the Model\n# ------------------------------\ntrainer_stats = trainer.train()\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T14:34:17.427633Z","iopub.execute_input":"2025-03-30T14:34:17.427982Z","iopub.status.idle":"2025-03-30T17:12:09.977103Z","shell.execute_reply.started":"2025-03-30T14:34:17.427948Z","shell.execute_reply":"2025-03-30T17:12:09.976250Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 11,330 | Num Epochs = 1 | Total steps = 60\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [60/60 2:37:18, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.018000</td>\n      <td>2.187816</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.065900</td>\n      <td>1.938652</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.954900</td>\n      <td>1.895473</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.948800</td>\n      <td>1.878576</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.885100</td>\n      <td>1.870739</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.930700</td>\n      <td>1.867373</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>â–ˆâ–ƒâ–‚â–â–â–</td></tr><tr><td>eval/runtime</td><td>â–ˆâ–ˆâ–‡â–â–…â–</td></tr><tr><td>eval/samples_per_second</td><td>â–â–â–â–â–â–</td></tr><tr><td>eval/steps_per_second</td><td>â–â–â–â–â–â–</td></tr><tr><td>train/epoch</td><td>â–â–â–‚â–‚â–„â–„â–…â–…â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/global_step</td><td>â–â–â–‚â–‚â–„â–„â–…â–…â–‡â–‡â–ˆâ–ˆâ–ˆ</td></tr><tr><td>train/grad_norm</td><td>â–ˆâ–ˆâ–â–‚â–‚â–</td></tr><tr><td>train/learning_rate</td><td>â–ˆâ–‡â–…â–„â–‚â–</td></tr><tr><td>train/loss</td><td>â–ˆâ–‚â–â–â–â–</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.86737</td></tr><tr><td>eval/runtime</td><td>1348.5062</td></tr><tr><td>eval/samples_per_second</td><td>1.05</td></tr><tr><td>eval/steps_per_second</td><td>0.525</td></tr><tr><td>total_flos</td><td>8410169003311104.0</td></tr><tr><td>train/epoch</td><td>0.04237</td></tr><tr><td>train/global_step</td><td>60</td></tr><tr><td>train/grad_norm</td><td>0.36734</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.9307</td></tr><tr><td>train_loss</td><td>2.13389</td></tr><tr><td>train_runtime</td><td>9467.5475</td></tr><tr><td>train_samples_per_second</td><td>0.051</td></tr><tr><td>train_steps_per_second</td><td>0.006</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">kind-darkness-7</strong> at: <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset/runs/tkwpf75p' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset/runs/tkwpf75p</a><br> View project at: <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20Medllama%208B%20on%20Medical%20Dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250330_143149-tkwpf75p/logs</code>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Call for_inference to enable Unslothâ€™s patched generation\nFastLanguageModel.for_inference(model)\n\n# -- OPTIONAL MONKEY-PATCH --\n# If you keep getting \"must call for_inference\" or need to bypass fast inference:\ndef generate_no_fast_inference(*args, **kwargs):\n    return model_lora.base_model.generate(*args, **kwargs)\n\n# Comment out the next line if you want to preserve Unsloth's speedups:\nmodel_lora.generate = generate_no_fast_inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T17:12:09.978175Z","iopub.execute_input":"2025-03-30T17:12:09.978406Z","iopub.status.idle":"2025-03-30T17:12:09.982652Z","shell.execute_reply.started":"2025-03-30T17:12:09.978385Z","shell.execute_reply":"2025-03-30T17:12:09.981933Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"question = \"\"\"I have osteoporosis mainly in the spine.I exercise 5 days a werk to help the situation. I don t know if there are any drugs that really help reverse the situation. What is your feeling about Prolia and its sidr effects. I have read them all. Also anything else you can suggest. I am in my 70s.\"\"\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=160,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T17:12:09.983531Z","iopub.execute_input":"2025-03-30T17:12:09.983766Z","iopub.status.idle":"2025-03-30T17:12:19.505468Z","shell.execute_reply.started":"2025-03-30T17:12:09.983747Z","shell.execute_reply":"2025-03-30T17:12:19.504598Z"}},"outputs":[{"name":"stdout","text":"\n<think>Hi, I am 65 years old and have been diagnosed with osteoporosis. I have been taking calcium and vitamin D supplements for the past 6 months. I have also been doing weight bearing exercises for the past 3 months. I have been advised to take a bisphosphonate medication. I am concerned about the side effects of this medication. I have read that it can cause osteonecrosis of the jaw. I am also concerned about the long term effects of this medication. I have also read that it can cause atrial fibrillation. I am also concerned about the long term effects of this medication. I am also concerned about the long term effects of this medication. I am also concerned about the long term effects of this medication. I am also concerned about the long term\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"question = \"\"\"I was hit on the forehead and went flying off a bed and hit the back of my head I have two indents in a line from my hairline to my eye, they are about 2 cm deep and a finger wide, they are about 5 cm apart, there isnt any bruising but its tender and I have a sharp pain there I had memory loss for about 12 hours, blurred vision, headache, nausea, dizziness, a bump on the back of my head and neck pain all the way down into my back I was wondering if I should get it checked, im avoiding it because I dont want anyone mad at the person because it was an accident, so I dont want to go unless necessary\"\"\"\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=320,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T17:12:19.506373Z","iopub.execute_input":"2025-03-30T17:12:19.506705Z","iopub.status.idle":"2025-03-30T17:12:37.390895Z","shell.execute_reply.started":"2025-03-30T17:12:19.506675Z","shell.execute_reply":"2025-03-30T17:12:37.389893Z"}},"outputs":[{"name":"stdout","text":"\n<think>Hi, I have a 2 year old son who has been having a fever for the past 3 days. He has a cough and a runny nose. He has been drinking a lot of fluids and eating well. He has been sleeping well and is not showing any signs of dehydration. He has been taking his medicine as prescribed. He has been having a lot of energy and is playing well. He has been having a lot of diarrhea. He has been having a lot of gas. He has been having a lot of stomach pain. He has been having a lot of vomiting. He has been having a lot of nausea. He has been having a lot of abdominal pain. He has been having a lot of abdominal distention. He has been having a lot of abdominal tenderness. He has been having a lot of abdominal rigidity. He has been having a lot of abdominal guarding. He has been having a lot of abdominal tenderness. He has been having a lot of abdominal rigidity. He has been having a lot of abdominal guarding. He has been having a lot of abdominal tenderness. He has been having a lot of abdominal rigidity. He has been having a lot of abdominal guarding. He has been having a lot of abdominal tenderness. He has been having a lot of abdominal rigidity. He has been having a lot of abdominal guarding. He has been having a lot of abdominal tenderness. He has been having a lot of abdominal rigidity. He has been having a lot of abdominal guarding. He has been having a lot of abdominal tenderness. He\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 7. Deploy the Fine-tuned Model on Hugging Face\n# ------------------------------\n# (Make sure you are logged in using your Hugging Face token)\nmodel_repo = \"KarthikNimmagadda/Medllama3-8B-Finetuned-Medical-Datasett\" # Update with your Hugging Face repository name\nmodel_lora.push_to_hub(model_repo, use_auth_token=hugging_face_token)\ntokenizer.push_to_hub(model_repo, use_auth_token=hugging_face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-30T17:12:37.391821Z","iopub.execute_input":"2025-03-30T17:12:37.392042Z","iopub.status.idle":"2025-03-30T17:12:49.533558Z","shell.execute_reply.started":"2025-03-30T17:12:37.392024Z","shell.execute_reply":"2025-03-30T17:12:49.532496Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/600 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"542fab7cf75249e6b2690ae9b41e4ed2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a19b07d87ea4476da6816494635c7e18"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3816bce76049455d9d93810134b82999"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/KarthikNimmagadda/Medllama3-8B-Finetuned-Medical-Datasett\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d55fadfbe0054d938724f0d959703f0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fc09ed37bb14198b8f8592fcfae1332"}},"metadata":{}}],"execution_count":16}]}