{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11134917,"sourceType":"datasetVersion","datasetId":6944884}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall torch -y\n!pip install torch --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:47:41.257293Z","iopub.execute_input":"2025-03-31T22:47:41.257483Z","iopub.status.idle":"2025-03-31T22:48:13.627719Z","shell.execute_reply.started":"2025-03-31T22:47:41.257464Z","shell.execute_reply":"2025-03-31T22:48:13.626529Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.6.0\nUninstalling torch-2.6.0:\n  Successfully uninstalled torch-2.6.0\nCollecting torch\n  Using cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.13.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nUsing cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\nInstalling collected packages: torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n# Install and upgrade Unsloth (and its latest version)\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:48:13.629040Z","iopub.execute_input":"2025-03-31T22:48:13.629438Z","iopub.status.idle":"2025-03-31T22:48:27.663854Z","shell.execute_reply.started":"2025-03-31T22:48:13.629398Z","shell.execute_reply":"2025-03-31T22:48:27.662721Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:48:27.664950Z","iopub.execute_input":"2025-03-31T22:48:27.665244Z","iopub.status.idle":"2025-03-31T22:48:43.300125Z","shell.execute_reply.started":"2025-03-31T22:48:27.665209Z","shell.execute_reply":"2025-03-31T22:48:43.299165Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"HF_TOKEN\")\nwnb_token = user_secrets.get_secret(\"wnb\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb\nrun = wandb.init(\n    project='Fine-tune MedAlpaca 7B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:48:43.301026Z","iopub.execute_input":"2025-03-31T22:48:43.301303Z","iopub.status.idle":"2025-03-31T22:48:56.207457Z","shell.execute_reply.started":"2025-03-31T22:48:43.301280Z","shell.execute_reply":"2025-03-31T22:48:56.206754Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarthik-nimmagadda\u001b[0m (\u001b[33mkarthik-nimmagadda-san-jose-state-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250331_224849-ya365q55</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset/runs/ya365q55' target=\"_blank\">youthful-moon-3</a></strong> to <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset/runs/ya365q55' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset/runs/ya365q55</a>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default \nload_in_4bit = True # Enables 4 bit quantization — a memory saving optimization \n\n# Load the DeepSeek R1 model and tokenizer using unsloth — imported using: from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name= \"medalpaca/medalpaca-7b\",  # Load the pre-trained gemma model (7B parameter version)\n    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n    token=hugging_face_token, # Use hugging face token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:48:56.209387Z","iopub.execute_input":"2025-03-31T22:48:56.209605Z","iopub.status.idle":"2025-03-31T22:50:57.858664Z","shell.execute_reply.started":"2025-03-31T22:48:56.209585Z","shell.execute_reply":"2025-03-31T22:50:57.858001Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"504894c221fd47218246818a740bc65e"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at medalpaca/medalpaca-7b were not used when initializing LlamaForCausalLM: ['model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq']\n- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\nYou are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Define prompt styles\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:50:57.860101Z","iopub.execute_input":"2025-03-31T22:50:57.860486Z","iopub.status.idle":"2025-03-31T22:50:57.865339Z","shell.execute_reply.started":"2025-03-31T22:50:57.860449Z","shell.execute_reply":"2025-03-31T22:50:57.864529Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Training prompt style updated to include both <think> and </think> tags:\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:50:57.866290Z","iopub.execute_input":"2025-03-31T22:50:57.866598Z","iopub.status.idle":"2025-03-31T22:50:57.883821Z","shell.execute_reply.started":"2025-03-31T22:50:57.866566Z","shell.execute_reply":"2025-03-31T22:50:57.883025Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load the dataset from Kaggle (assumes the dataset has fields: \"context\", \"input\", \"output\")\ndataset = load_dataset(\"/kaggle/input/my-datasets/\", split=\"train\", trust_remote_code=True)\n\n# Split into training (80%), and temp (20%)\nsplit_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset[\"train\"]\ntemp_dataset = split_dataset[\"test\"]\n\n# Split the temporary dataset equally into evaluation (10%) and test (10%)\ntemp_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\neval_dataset = temp_split[\"train\"]\ntest_dataset = temp_split[\"test\"]\n\nprint(\"Training samples:\", len(train_dataset))\nprint(\"Evaluation samples:\", len(eval_dataset))\nprint(\"Test samples:\", len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:50:57.884632Z","iopub.execute_input":"2025-03-31T22:50:57.884832Z","iopub.status.idle":"2025-03-31T22:50:58.091899Z","shell.execute_reply.started":"2025-03-31T22:50:57.884814Z","shell.execute_reply":"2025-03-31T22:50:58.090929Z"}},"outputs":[{"name":"stdout","text":"Training samples: 11330\nEvaluation samples: 1416\nTest samples: 1417\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 2. Format the Datasets for Fine-tuning\n# ------------------------------\nEOS_TOKEN = tokenizer.eos_token  # End-of-sequence token\n\ndef formatting_prompts_func(examples):\n    inputs_list = examples[\"context\"]  # Medical question\n    cots_list = examples[\"input\"]      # Chain-of-thought reasoning\n    outputs_list = examples[\"output\"]  # Final answer\n    texts = []\n    for inp, cot, out in zip(inputs_list, cots_list, outputs_list):\n        text = train_prompt_style.format(inp, cot, out) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Map the formatting function over all splits\ntrain_dataset_formatted = train_dataset.map(formatting_prompts_func, batched=True)\neval_dataset_formatted  = eval_dataset.map(formatting_prompts_func, batched=True)\ntest_dataset_formatted  = test_dataset.map(formatting_prompts_func, batched=True)\n\n# (Optional) Inspect one formatted example\nprint(\"Formatted prompt example:\\n\", train_dataset_formatted[\"text\"][6])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:50:58.092779Z","iopub.execute_input":"2025-03-31T22:50:58.092983Z","iopub.status.idle":"2025-03-31T22:50:58.181675Z","shell.execute_reply.started":"2025-03-31T22:50:58.092965Z","shell.execute_reply":"2025-03-31T22:50:58.180824Z"}},"outputs":[{"name":"stdout","text":"Formatted prompt example:\n Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\nWhat causes pain in right upper jaw and headache?\n\n### Response:\n<think>\nI have a blister filled bump on my upper front gum line I have been taking antibiotics and 2 days ago the bump popped and it drained yellow puss out  and then blood my tooth is broken off in the gum line the tooth does not hurt im curious is to what is going on\n</think>\nThanks for your query, I have gone through your query.The blister bump on the gums can be a pus discharging sinus tract secondary to an infected tooth or a fractured tooth. Nothing to be panic, complete the course of antibiotics particularly a combination of amoxicillin and metronidazole. Then get a radiograph done to check the amount of tooth structure remaining and surrounding bone level. If sufficient bone and tooth structure is there, then it can be restored with root canal treatment otherwise, it has to be removed.I hope my answer will help you, take care.</s>\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 3. Prepare for Fine-tuning with LoRA\n# ------------------------------\n# Apply LoRA to the model\nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# Monkey-patch for safe training (restores model.generate if needed)\ndef safe_for_training(model, use_gradient_checkpointing=True):\n    if hasattr(model, \"_unwrapped_old_generate\"):\n        model.generate = model._unwrapped_old_generate\n        try:\n            del model._unwrapped_old_generate\n        except AttributeError:\n            pass\n    return model\n\nmodel_lora.for_training = lambda: safe_for_training(model_lora)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:50:58.182434Z","iopub.execute_input":"2025-03-31T22:50:58.182758Z","iopub.status.idle":"2025-03-31T22:51:04.171919Z","shell.execute_reply.started":"2025-03-31T22:50:58.182730Z","shell.execute_reply":"2025-03-31T22:51:04.171249Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 4. Initialize the Fine-tuning Trainer\n# ------------------------------\ntrainer = SFTTrainer(\n    model=model_lora,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset_formatted,\n    eval_dataset=eval_dataset_formatted,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,\n        warmup_steps=5,\n        max_steps=30,\n        evaluation_strategy=\"steps\",  # Enable periodic evaluation\n        eval_steps=10,                # Evaluate every 10 steps\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:51:04.172590Z","iopub.execute_input":"2025-03-31T22:51:04.172797Z","iopub.status.idle":"2025-03-31T22:51:04.591730Z","shell.execute_reply.started":"2025-03-31T22:51:04.172779Z","shell.execute_reply":"2025-03-31T22:51:04.590756Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# 5. Fine-tune the Model\n# ------------------------------\ntrainer_stats = trainer.train()\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T22:51:04.592636Z","iopub.execute_input":"2025-03-31T22:51:04.592952Z","iopub.status.idle":"2025-04-01T00:13:17.633298Z","shell.execute_reply.started":"2025-03-31T22:51:04.592918Z","shell.execute_reply":"2025-04-01T00:13:17.632448Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 11,330 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 39,976,960/7,000,000,000 (0.57% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 1:21:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.541100</td>\n      <td>2.091575</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.884600</td>\n      <td>1.705612</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.729000</td>\n      <td>1.659819</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▂▁</td></tr><tr><td>eval/runtime</td><td>▁▃█</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▅▅███</td></tr><tr><td>train/global_step</td><td>▁▁▅▅███</td></tr><tr><td>train/grad_norm</td><td>█▁▇</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.65982</td></tr><tr><td>eval/runtime</td><td>1394.9219</td></tr><tr><td>eval/samples_per_second</td><td>1.015</td></tr><tr><td>eval/steps_per_second</td><td>0.508</td></tr><tr><td>total_flos</td><td>4516857114624000.0</td></tr><tr><td>train/epoch</td><td>0.02118</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>0.41449</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>1.729</td></tr><tr><td>train_loss</td><td>2.05157</td></tr><tr><td>train_runtime</td><td>4929.524</td></tr><tr><td>train_samples_per_second</td><td>0.049</td></tr><tr><td>train_steps_per_second</td><td>0.006</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">youthful-moon-3</strong> at: <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset/runs/ya365q55' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset/runs/ya365q55</a><br> View project at: <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20MedAlpaca%207B%20on%20Medical%20Dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250331_224849-ya365q55/logs</code>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Call for_inference to enable Unsloth’s patched generation\nFastLanguageModel.for_inference(model)\n\n# -- OPTIONAL MONKEY-PATCH --\n# If you keep getting \"must call for_inference\" or need to bypass fast inference:\ndef generate_no_fast_inference(*args, **kwargs):\n    return model_lora.base_model.generate(*args, **kwargs)\n\n# Comment out the next line if you want to preserve Unsloth's speedups:\nmodel_lora.generate = generate_no_fast_inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:13:17.634350Z","iopub.execute_input":"2025-04-01T00:13:17.634572Z","iopub.status.idle":"2025-04-01T00:13:17.638984Z","shell.execute_reply.started":"2025-04-01T00:13:17.634547Z","shell.execute_reply":"2025-04-01T00:13:17.638006Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"question = \"\"\"I have osteoporosis mainly in the spine.I exercise 5 days a werk to help the situation. I don t know if there are any drugs that really help reverse the situation. What is your feeling about Prolia and its sidr effects. I have read them all. Also anything else you can suggest. I am in my 70s.\"\"\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=160,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:13:17.639895Z","iopub.execute_input":"2025-04-01T00:13:17.640208Z","iopub.status.idle":"2025-04-01T00:13:27.224043Z","shell.execute_reply.started":"2025-04-01T00:13:17.640152Z","shell.execute_reply":"2025-04-01T00:13:27.223057Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nI have osteoporosis mainly in the spine.I exercise 5 days a werk to help the situation. I don t know if there are any drugs that really help reverse the situation. What is your feeling about Prolia and its sidr effects. I have read them all. Also anything else you can suggest. I am in my 70s.\n</think>\nHi, I am Chat Doctor. I have read your question and understand your concerns. I will try to help you.\n<think>\nI have osteoporosis mainly in the spine.I exercise 5 days a werk to help the situation. I don t know if there are any drugs that really help reverse the situation. What is your\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"question = \"\"\"I was hit on the forehead and went flying off a bed and hit the back of my head I have two indents in a line from my hairline to my eye, they are about 2 cm deep and a finger wide, they are about 5 cm apart, there isnt any bruising but its tender and I have a sharp pain there I had memory loss for about 12 hours, blurred vision, headache, nausea, dizziness, a bump on the back of my head and neck pain all the way down into my back I was wondering if I should get it checked, im avoiding it because I dont want anyone mad at the person because it was an accident, so I dont want to go unless necessary\"\"\"\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=320,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:13:27.225022Z","iopub.execute_input":"2025-04-01T00:13:27.225339Z","iopub.status.idle":"2025-04-01T00:13:45.261022Z","shell.execute_reply.started":"2025-04-01T00:13:27.225313Z","shell.execute_reply":"2025-04-01T00:13:45.259964Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nI was hit on the forehead and went flying off a bed and hit the back of my head I have two indents in a line from my hairline to my eye, they are about 2 cm deep and a finger wide, they are about 5 cm apart, there isnt any bruising but its tender and I have a sharp pain there I had memory loss for about 12 hours, blurred vision, headache, nausea, dizziness, a bump on the back of my head and neck pain all the way down into my back I was wondering if I should get it checked, im avoiding it because I dont want anyone mad at the person because it was an accident, so I dont want to go unless necessary\n</think>\nHi,\nI have gone through your query and I understand your concern.\nYou have sustained a head injury. It is important to get a CT scan done to rule out any internal injuries.\nI would suggest you to get a CT scan done.\nI hope this helps.\nPlease write back if you have any further queries.\nRegards,\n\n<think>\nI was hit on the forehead and went flying off a bed and hit the back of my head I have two indents in a line from my hairline to my eye, they are about 2 cm deep and a finger wide, they are about 5 cm apart, there isnt any bruising but its tender and I have a sharp pain there I had memory loss for\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 7. Deploy the Fine-tuned Model on Hugging Face\n# ------------------------------\n# (Make sure you are logged in using your Hugging Face token)\nmodel_repo = \"KarthikNimmagadda/MedAlpaca-7B-Finetuned-Medical-Datasett\" # Update with your Hugging Face repository name\nmodel_lora.push_to_hub(model_repo, use_auth_token=hugging_face_token)\ntokenizer.push_to_hub(model_repo, use_auth_token=hugging_face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:13:45.262011Z","iopub.execute_input":"2025-04-01T00:13:45.262272Z","iopub.status.idle":"2025-04-01T00:13:53.343394Z","shell.execute_reply.started":"2025-04-01T00:13:45.262248Z","shell.execute_reply":"2025-04-01T00:13:53.342701Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/574 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a082d3e9015c4ee1a2a3b31f4197a43c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed3a7b2a0c2948369acf4150ae5fb5fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/160M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26901c1a0974301903c3f403b6dcd0f"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/KarthikNimmagadda/MedAlpaca-7B-Finetuned-Medical-Datasett\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f47a9cff0e54423904f5fa985b13763"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b1fb0c27e554610a81393ab4dff6162"}},"metadata":{}}],"execution_count":16}]}