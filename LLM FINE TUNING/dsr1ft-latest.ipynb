{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11134917,"sourceType":"datasetVersion","datasetId":6944884}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall torch -y\n!pip install torch --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:41:05.916006Z","iopub.execute_input":"2025-04-01T00:41:05.916374Z","iopub.status.idle":"2025-04-01T00:41:38.871355Z","shell.execute_reply.started":"2025-04-01T00:41:05.916344Z","shell.execute_reply":"2025-04-01T00:41:38.870294Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.6.0\nUninstalling torch-2.6.0:\n  Successfully uninstalled torch-2.6.0\nCollecting torch\n  Using cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.13.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nUsing cached torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\nInstalling collected packages: torch\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\ntorchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.6.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n# Install and upgrade Unsloth (and its latest version)\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:41:38.872627Z","iopub.execute_input":"2025-04-01T00:41:38.872939Z","iopub.status.idle":"2025-04-01T00:41:52.759090Z","shell.execute_reply.started":"2025-04-01T00:41:38.872914Z","shell.execute_reply":"2025-04-01T00:41:52.757977Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:41:52.760971Z","iopub.execute_input":"2025-04-01T00:41:52.761353Z","iopub.status.idle":"2025-04-01T00:42:08.696476Z","shell.execute_reply.started":"2025-04-01T00:41:52.761311Z","shell.execute_reply":"2025-04-01T00:42:08.695651Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\nü¶• Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"HF_TOKEN\")\nwnb_token = user_secrets.get_secret(\"wnb\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb\nrun = wandb.init(\n    project='Fine-tune DeepSeek R1 Distill Llama 8B on Medical Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:42:08.697798Z","iopub.execute_input":"2025-04-01T00:42:08.698113Z","iopub.status.idle":"2025-04-01T00:42:21.757271Z","shell.execute_reply.started":"2025-04-01T00:42:08.698079Z","shell.execute_reply":"2025-04-01T00:42:21.756599Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkarthik-nimmagadda\u001b[0m (\u001b[33mkarthik-nimmagadda-san-jose-state-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250401_004215-mdd4nv1a</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset/runs/mdd4nv1a' target=\"_blank\">decent-gorge-1</a></strong> to <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset/runs/mdd4nv1a' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset/runs/mdd4nv1a</a>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Set parameters\nmax_seq_length = 2048 # Define the maximum sequence length a model can handle (i.e. how many tokens can be processed at once)\ndtype = None # Set to default \nload_in_4bit = True # Enables 4 bit quantization ‚Äî a memory saving optimization \n\n# Load the DeepSeek R1 model and tokenizer using unsloth ‚Äî imported using: from unsloth import FastLanguageModel\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name= \"unsloth/DeepSeek-R1-Distill-Llama-8B\",  # Load the pre-trained gemma model (7B parameter version)\n    max_seq_length=max_seq_length, # Ensure the model can process up to 2048 tokens at once\n    dtype=dtype, # Use the default data type (e.g., FP16 or BF16 depending on hardware support)\n    load_in_4bit=load_in_4bit, # Load the model in 4-bit quantization to save memory\n    token=hugging_face_token, # Use hugging face token\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:42:21.757966Z","iopub.execute_input":"2025-04-01T00:42:21.758217Z","iopub.status.idle":"2025-04-01T00:43:03.373945Z","shell.execute_reply.started":"2025-04-01T00:42:21.758185Z","shell.execute_reply":"2025-04-01T00:43:03.373033Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.50.3.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"398f763a597b49bba8f54fe1a4d6209b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/236 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4c799eb60d34d9386955ab04346c96b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ec1fe61a3049d0b20c8c3fa9695e64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b91a5cb25e64874ae4c9f88a5003687"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3aad78fdc04e5886642dc1cfba274d"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Define prompt styles\nprompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:43:03.375011Z","iopub.execute_input":"2025-04-01T00:43:03.375337Z","iopub.status.idle":"2025-04-01T00:43:03.379991Z","shell.execute_reply.started":"2025-04-01T00:43:03.375306Z","shell.execute_reply":"2025-04-01T00:43:03.379130Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Training prompt style updated to include both <think> and </think> tags:\ntrain_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\n{}\n\n### Response:\n<think>\n{}\n</think>\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:43:03.380961Z","iopub.execute_input":"2025-04-01T00:43:03.381288Z","iopub.status.idle":"2025-04-01T00:43:03.521849Z","shell.execute_reply.started":"2025-04-01T00:43:03.381256Z","shell.execute_reply":"2025-04-01T00:43:03.520959Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Load the dataset from Kaggle (assumes the dataset has fields: \"context\", \"input\", \"output\")\ndataset = load_dataset(\"/kaggle/input/my-datasets/\", split=\"train\", trust_remote_code=True)\n\n# Split into training (80%), and temp (20%)\nsplit_dataset = dataset.train_test_split(test_size=0.2, seed=42)\ntrain_dataset = split_dataset[\"train\"]\ntemp_dataset = split_dataset[\"test\"]\n\n# Split the temporary dataset equally into evaluation (10%) and test (10%)\ntemp_split = temp_dataset.train_test_split(test_size=0.5, seed=42)\neval_dataset = temp_split[\"train\"]\ntest_dataset = temp_split[\"test\"]\n\nprint(\"Training samples:\", len(train_dataset))\nprint(\"Evaluation samples:\", len(eval_dataset))\nprint(\"Test samples:\", len(test_dataset))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:43:03.522741Z","iopub.execute_input":"2025-04-01T00:43:03.523088Z","iopub.status.idle":"2025-04-01T00:43:03.642264Z","shell.execute_reply.started":"2025-04-01T00:43:03.523057Z","shell.execute_reply":"2025-04-01T00:43:03.641482Z"}},"outputs":[{"name":"stdout","text":"Training samples: 11330\nEvaluation samples: 1416\nTest samples: 1417\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# 2. Format the Datasets for Fine-tuning\n# ------------------------------\nEOS_TOKEN = tokenizer.eos_token  # End-of-sequence token\n\ndef formatting_prompts_func(examples):\n    inputs_list = examples[\"context\"]  # Medical question\n    cots_list = examples[\"input\"]      # Chain-of-thought reasoning\n    outputs_list = examples[\"output\"]  # Final answer\n    texts = []\n    for inp, cot, out in zip(inputs_list, cots_list, outputs_list):\n        text = train_prompt_style.format(inp, cot, out) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Map the formatting function over all splits\ntrain_dataset_formatted = train_dataset.map(formatting_prompts_func, batched=True)\neval_dataset_formatted  = eval_dataset.map(formatting_prompts_func, batched=True)\ntest_dataset_formatted  = test_dataset.map(formatting_prompts_func, batched=True)\n\n# (Optional) Inspect one formatted example\nprint(\"Formatted prompt example:\\n\", train_dataset_formatted[\"text\"][6])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:43:03.644292Z","iopub.execute_input":"2025-04-01T00:43:03.644524Z","iopub.status.idle":"2025-04-01T00:43:04.178091Z","shell.execute_reply.started":"2025-04-01T00:43:03.644504Z","shell.execute_reply":"2025-04-01T00:43:04.177224Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11330 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f9d5a8c8224424e9f03d8b5b25e4cb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1416 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76c5f00cded6455d9de1bcad3e21cf0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1417 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3164e316864543a8ae6aedd55693f59d"}},"metadata":{}},{"name":"stdout","text":"Formatted prompt example:\n Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning.\nPlease answer the following medical question.\n\n### Question:\nWhat causes pain in right upper jaw and headache?\n\n### Response:\n<think>\nI have a blister filled bump on my upper front gum line I have been taking antibiotics and 2 days ago the bump popped and it drained yellow puss out  and then blood my tooth is broken off in the gum line the tooth does not hurt im curious is to what is going on\n</think>\nThanks for your query, I have gone through your query.The blister bump on the gums can be a pus discharging sinus tract secondary to an infected tooth or a fractured tooth. Nothing to be panic, complete the course of antibiotics particularly a combination of amoxicillin and metronidazole. Then get a radiograph done to check the amount of tooth structure remaining and surrounding bone level. If sufficient bone and tooth structure is there, then it can be restored with root canal treatment otherwise, it has to be removed.I hope my answer will help you, take care.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# 3. Prepare for Fine-tuning with LoRA\n# ------------------------------\n# Apply LoRA to the model\nmodel_lora = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=False,\n    loftq_config=None,\n)\n\n# Monkey-patch for safe training (restores model.generate if needed)\ndef safe_for_training(model, use_gradient_checkpointing=True):\n    if hasattr(model, \"_unwrapped_old_generate\"):\n        model.generate = model._unwrapped_old_generate\n        try:\n            del model._unwrapped_old_generate\n        except AttributeError:\n            pass\n    return model\n\nmodel_lora.for_training = lambda: safe_for_training(model_lora)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:43:04.179619Z","iopub.execute_input":"2025-04-01T00:43:04.179961Z","iopub.status.idle":"2025-04-01T00:43:10.208856Z","shell.execute_reply.started":"2025-04-01T00:43:04.179925Z","shell.execute_reply":"2025-04-01T00:43:10.207897Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# 4. Initialize the Fine-tuning Trainer\n# ------------------------------\ntrainer = SFTTrainer(\n    model=model_lora,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset_formatted,\n    eval_dataset=eval_dataset_formatted,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=2,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        num_train_epochs=1,\n        warmup_steps=5,\n        max_steps=30,\n        evaluation_strategy=\"steps\",  # Enable periodic evaluation\n        eval_steps=10,                # Evaluate every 10 steps\n        learning_rate=2e-4,\n        fp16=not is_bfloat16_supported(),\n        bf16=is_bfloat16_supported(),\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:43:10.209878Z","iopub.execute_input":"2025-04-01T00:43:10.210209Z","iopub.status.idle":"2025-04-01T00:43:20.188030Z","shell.execute_reply.started":"2025-04-01T00:43:10.210160Z","shell.execute_reply":"2025-04-01T00:43:20.187253Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/11330 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58ae95f120c64cab886b8e3276e695dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/1416 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31eb6cba1a714ffeb2f8c696d80e7700"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# 5. Fine-tune the Model\n# ------------------------------\ntrainer_stats = trainer.train()\nwandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T00:43:20.188973Z","iopub.execute_input":"2025-04-01T00:43:20.189226Z","iopub.status.idle":"2025-04-01T02:02:26.802532Z","shell.execute_reply.started":"2025-04-01T00:43:20.189195Z","shell.execute_reply":"2025-04-01T02:02:26.801800Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 11,330 | Num Epochs = 1 | Total steps = 30\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [30/30 1:18:38, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>3.184100</td>\n      <td>2.372672</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.276300</td>\n      <td>2.152044</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.177900</td>\n      <td>2.117664</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\nUsing gradient accumulation will be very slightly less accurate.\nRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñÉ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñÖ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>2.11766</td></tr><tr><td>eval/runtime</td><td>1348.0076</td></tr><tr><td>eval/samples_per_second</td><td>1.05</td></tr><tr><td>eval/steps_per_second</td><td>0.525</td></tr><tr><td>total_flos</td><td>4285051111931904.0</td></tr><tr><td>train/epoch</td><td>0.02118</td></tr><tr><td>train/global_step</td><td>30</td></tr><tr><td>train/grad_norm</td><td>0.43229</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>2.1779</td></tr><tr><td>train_loss</td><td>2.54609</td></tr><tr><td>train_runtime</td><td>4742.683</td></tr><tr><td>train_samples_per_second</td><td>0.051</td></tr><tr><td>train_steps_per_second</td><td>0.006</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">decent-gorge-1</strong> at: <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset/runs/mdd4nv1a' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset/runs/mdd4nv1a</a><br> View project at: <a href='https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset' target=\"_blank\">https://wandb.ai/karthik-nimmagadda-san-jose-state-university/Fine-tune%20DeepSeek%20R1%20Distill%20Llama%208B%20on%20Medical%20Dataset</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250401_004215-mdd4nv1a/logs</code>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Call for_inference to enable Unsloth‚Äôs patched generation\nFastLanguageModel.for_inference(model)\n\n# -- OPTIONAL MONKEY-PATCH --\n# If you keep getting \"must call for_inference\" or need to bypass fast inference:\ndef generate_no_fast_inference(*args, **kwargs):\n    return model_lora.base_model.generate(*args, **kwargs)\n\n# Comment out the next line if you want to preserve Unsloth's speedups:\nmodel_lora.generate = generate_no_fast_inference","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T02:02:26.803682Z","iopub.execute_input":"2025-04-01T02:02:26.804057Z","iopub.status.idle":"2025-04-01T02:02:26.808840Z","shell.execute_reply.started":"2025-04-01T02:02:26.804022Z","shell.execute_reply":"2025-04-01T02:02:26.807871Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"question = \"\"\"I have osteoporosis mainly in the spine.I exercise 5 days a werk to help the situation. I don t know if there are any drugs that really help reverse the situation. What is your feeling about Prolia and its sidr effects. I have read them all. Also anything else you can suggest. I am in my 70s.\"\"\"\n\n# Load the inference model using FastLanguageModel (Unsloth optimizes for speed)\nFastLanguageModel.for_inference(model_lora)  # Unsloth has 2x faster inference!\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=160,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T02:02:26.809548Z","iopub.execute_input":"2025-04-01T02:02:26.809746Z","iopub.status.idle":"2025-04-01T02:02:37.007624Z","shell.execute_reply.started":"2025-04-01T02:02:26.809727Z","shell.execute_reply":"2025-04-01T02:02:37.006356Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nHello Dr. I have a 5 year old boy with type 1 diabetes. His blood sugar levels are under control with insulin. He has developed a blod sugar level of 6.8 on 3 occasions. His blood sugar levels are usually under control. Should I be worried about this? Is there any risk of developing type 2 diabetes in future? Please advise.\n</think>\nI think that you should not worry about your son developing type 2 diabetes in the future. Type 1 diabetes is an autoimmune disease and is not related to type 2 diabetes. The blood sugar levels that you are referring to are not indicative of type 2 diabetes. However, it is important to monitor your son's blood sugar levels carefully and consult with your pediatric endocrinologist.<ÔΩúend‚ñÅof‚ñÅsentenceÔΩú>\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"question = \"\"\"I was hit on the forehead and went flying off a bed and hit the back of my head I have two indents in a line from my hairline to my eye, they are about 2 cm deep and a finger wide, they are about 5 cm apart, there isnt any bruising but its tender and I have a sharp pain there I had memory loss for about 12 hours, blurred vision, headache, nausea, dizziness, a bump on the back of my head and neck pain all the way down into my back I was wondering if I should get it checked, im avoiding it because I dont want anyone mad at the person because it was an accident, so I dont want to go unless necessary\"\"\"\n\n# Tokenize the input question with a specific prompt format and move it to the GPU\ninputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n# Generate a response using LoRA fine-tuned model with specific parameters\noutputs = model_lora.generate(\n    input_ids=inputs.input_ids,          # Tokenized input IDs\n    attention_mask=inputs.attention_mask, # Attention mask for padding handling\n    max_new_tokens=320,                  # Maximum length for generated response\n    use_cache=True,                        # Enable cache for efficient generation\n)\n\n# Decode the generated response from tokenized format to readable text\nresponse = tokenizer.batch_decode(outputs)\n\n# Extract and print only the model's response part after \"### Response:\"\nprint(response[0].split(\"### Response:\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T02:02:37.009057Z","iopub.execute_input":"2025-04-01T02:02:37.009422Z","iopub.status.idle":"2025-04-01T02:02:56.435393Z","shell.execute_reply.started":"2025-04-01T02:02:37.009386Z","shell.execute_reply":"2025-04-01T02:02:56.434387Z"}},"outputs":[{"name":"stdout","text":"\n<think>\nI am 21 years old, I was hit in the head with a baseball bat and fell down and hit my head on the ground. I have a lump on the back of my head and neck pain. I have also been having severe headaches and I feel like I am going to pass out. I have been to the doctor and they said it was just a mild contusion and to take it easy. I have a large bump on the back of my head and I feel like it is going to get bigger. I am worried that it may be a blood clot or something serious. I have been taking ibuprofen and it helps a little but I still feel like I have a lump on my head. I am going to have a MRI tomorrow and I am really worried about it. I hope everything is going to be okay. I have also been having bad headaches and I feel like it is going to get worse. I hope everything is going to be okay and that it is not something serious. I am going to go to the doctor tomorrow and I hope everything is going to be okay. Thank you.\n</think>\nHi I am 21 years old and I was hit in the head with a baseball bat and fell down and hit my head on the ground. I have a lump on the back of my head and neck pain. I have also been having severe headaches and I feel like I am going to pass out. I have been to the doctor and they said it was just a mild contusion and to take it easy. I have a large bump on the back of my\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# 7. Deploy the Fine-tuned Model on Hugging Face\n# ------------------------------\n# (Make sure you are logged in using your Hugging Face token)\nmodel_repo = \"KarthikNimmagadda/DeepSeek-Latest-Finetuned-Medical-Dataset\" # Update with your Hugging Face repository name\nmodel_lora.push_to_hub(model_repo, use_auth_token=hugging_face_token)\ntokenizer.push_to_hub(model_repo, use_auth_token=hugging_face_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-01T02:07:29.100675Z","iopub.execute_input":"2025-04-01T02:07:29.100984Z","iopub.status.idle":"2025-04-01T02:07:39.552732Z","shell.execute_reply.started":"2025-04-01T02:07:29.100960Z","shell.execute_reply":"2025-04-01T02:07:39.551944Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/636 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ca95395445b4a4da1d52f72232b895e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db358322c1954cd3a7d3060c4f7de3d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/168M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fbe9a673f3f4a9cb79b56b10f56524f"}},"metadata":{}},{"name":"stdout","text":"Saved model to https://huggingface.co/KarthikNimmagadda/DeepSeek-Latest-Finetuned-Medical-Dataset\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f40c68a5e90343dfa257297890a62da0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1600fbbe3d942ada6480c1cfe7b1a28"}},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}