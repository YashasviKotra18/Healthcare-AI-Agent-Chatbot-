{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3e5d9f88-136d-480a-8c3c-8c8a03b65eb9",
   "metadata": {},
   "source": [
    "RAW DATA INGESTION FROM LOCAL TO GCP BUCKET\n",
    "\n",
    "created a service account from IAM service , from GCP \n",
    "\n",
    "for loading the raw files from local to gcp bucket,\n",
    "\n",
    "use the credetials from service account , in local jupyter notebook where the files will be dumped \n",
    "\n",
    "run the script for loading the files from loacl folder to loading it to GCP bucket (raw_dataset_genai)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c28919-b630-4bee-a125-ad2695c3dffe",
   "metadata": {},
   "source": [
    "### Data Ingestion Process Report\n",
    "\n",
    "#### Objective\n",
    "This outlines the data ingestion process designed for a data engineering pipeline that collects raw datasets from multiple source directories and ingests them into Google Cloud Storage (GCS). The ingestion process prepares data for subsequent data processing tasks in LLM MODEL BUILDING, including dataset partitioning for training, validation, and testing.\n",
    "\n",
    "#### Process Overview\n",
    "The data ingestion script automates the collection, validation, and storage of raw datasets from specified local directories into a GCS bucket. This process serves as the foundational step in data engineering, ensuring that data is readily accessible and structured within the cloud environment for further analysis and machine learning tasks.\n",
    "\n",
    "The ingestion process encompasses the following key steps:\n",
    "\n",
    "1. **Configuration and Authentication**  \n",
    "   - The ingestion pipeline begins by establishing a connection to GCS using a service account credential. This setup provides secure and authorized access to the cloud storage, allowing files to be uploaded without interruption.\n",
    "   - The service account JSON file (`gc_key.json`) is loaded, and a `storage.Client` is initialized to access the specified GCS bucket.\n",
    "\n",
    "2. **Directory and File Validation**  \n",
    "   - Each source directory and its corresponding blob prefix (GCS folder path) are paired. For each directory:\n",
    "     - The script verifies the directory's existence.\n",
    "     - It then lists all files in the directory and confirms whether files are present. This validation ensures that directories are not processed if they are empty, improving efficiency.\n",
    "   - A message is logged if no files are found in a directory, and the script proceeds to the next directory.\n",
    "\n",
    "3. **File Upload and Cloud Storage Organization**  \n",
    "   - For each file in a valid directory, a blob path is created within the GCS bucket using the specified prefix. This organizes the files in GCS, creating a well-structured cloud storage system.\n",
    "   - Each file is uploaded to its designated location in GCS. The script logs each successful upload, providing traceability and transparency in data ingestion.\n",
    "\n",
    "4. **File Deletion for Storage Management**  \n",
    "   - After each successful upload, the local file is deleted. This step helps manage storage on the local machine, retaining only the files that have not been uploaded. It also prevents duplication and ensures that local storage is freed up for future data ingestion tasks.\n",
    "   - Deletion is performed only if the upload is successful, adding a level of robustness to prevent accidental data loss.\n",
    "\n",
    "#### Logging and Error Handling\n",
    "The ingestion process incorporates comprehensive logging and error handling to maintain a smooth pipeline:\n",
    "- **Logging**: Each key step is logged, including successful uploads, file deletions, skipped directories, and any errors encountered.\n",
    "- **Error Handling**: Exceptions are caught and logged, especially during cloud client initialization and file uploads. This ensures that the process continues smoothly without stopping due to minor issues.\n",
    "\n",
    "\n",
    "#### Conclusion\n",
    "This data ingestion process efficiently collects, validates, and ingests raw datasets from local storage to cloud storage. With its built-in logging, error handling, and file management, this pipeline sets the foundation for a well-organized, scalable data engineering environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048d6c59-c6a3-43f0-b01a-4deaaf1ce650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google-cloud-storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb26c2d-a162-45f4-8be4-5cf4b3144511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd15da-59c7-4a42-be57-bde8cb005864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3adba2f-2c4c-4a20-a46a-60ae2e1dbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c15f7f-9e5c-4a51-8fc1-70ab5ba3a427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e50af77b-5094-489c-9ae7-b59a61dac37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files_to_gcs_multiple_sources(bucket_name, local_directories, blob_prefixes):\n",
    "    \"\"\"\n",
    "    Ingests data by uploading files from multiple local directories to respective locations in a GCS bucket.\n",
    "    Includes error handling, logging, and file validations as part of a data engineering pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    - bucket_name (str): The name of the GCS bucket.\n",
    "    - local_directories (list): List of local directories containing files to upload.\n",
    "    - blob_prefixes (list): List of corresponding GCS blob prefixes for each local directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # Path to your service account key file\n",
    "    service_account_path = 'gc_key.json'  # Replace with your service account JSON path\n",
    "\n",
    "    # Create a credentials object and initialize the GCS client\n",
    "    try:\n",
    "        credentials = service_account.Credentials.from_service_account_file(service_account_path)\n",
    "        storage_client = storage.Client(credentials=credentials)\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        logger.info(\"Google Cloud Storage client initialized.\")\n",
    "    except Exception as e:\n",
    "        logger.error(\"Failed to initialize Google Cloud Storage client.\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # Check if lists have the same length\n",
    "    if len(local_directories) != len(blob_prefixes):\n",
    "        logger.error(\"The number of local directories and blob prefixes must match.\")\n",
    "        return\n",
    "\n",
    "    # Process each local directory and corresponding blob prefix\n",
    "    for local_directory, blob_prefix in zip(local_directories, blob_prefixes):\n",
    "        logger.info(f\"Processing files from '{local_directory}' to GCS prefix '{blob_prefix}'...\")\n",
    "\n",
    "        # Validate if the directory exists and contains files\n",
    "        if not os.path.isdir(local_directory):\n",
    "            logger.warning(f\"Directory '{local_directory}' does not exist. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # List files in the directory\n",
    "        files = [f for f in os.listdir(local_directory) if os.path.isfile(os.path.join(local_directory, f))]\n",
    "        if not files:\n",
    "            logger.info(f\"No files found in '{local_directory}'. Skipping to next directory.\")\n",
    "            continue\n",
    "\n",
    "        # Process each file in the directory\n",
    "        for file_name in files:\n",
    "            local_file_path = os.path.join(local_directory, file_name)\n",
    "            blob_path = os.path.join(blob_prefix, file_name)\n",
    "            blob = bucket.blob(blob_path)\n",
    "\n",
    "            try:\n",
    "                # Attempt to upload the file\n",
    "                blob.upload_from_filename(local_file_path)\n",
    "                logger.info(f\"Uploaded '{file_name}' from '{local_directory}' to 'gs://{bucket_name}/{blob_path}'\")\n",
    "\n",
    "                # Optionally, delete the local file after upload (commented out by default)\n",
    "                os.remove(local_file_path)\n",
    "                \n",
    "                logger.info(f\"Deleted local file '{local_file_path}' after upload.\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to upload '{file_name}' from '{local_directory}'\", exc_info=True)\n",
    "\n",
    "    logger.info(\"Ingestion process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43052fd7-dda6-4719-95c4-fd62e0c1e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use the function with necessary parameters\n",
    "bucket_name = 'raw_dataset_genai'  # Replace with your GCS bucket name\n",
    "local_directories = [\n",
    "    'DATASET/iqlinic/',  # Replace with actual folder paths\n",
    "    'DATASET/healthcaremagic/',\n",
    "    'DATASET/mimic/',\n",
    "    'DATASET/misc/'\n",
    "]\n",
    "blob_prefixes = [\n",
    "    'iqlinic',  # Replace with corresponding GCS folder paths (or prefixes)\n",
    "    'heathcaremagic',\n",
    "    'mimic',\n",
    "    'misc'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23cdba0-7db3-4104-8f78-dab044779444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-05 12:57:21,527 - INFO - Google Cloud Storage client initialized.\n",
      "2024-11-05 12:57:21,528 - INFO - Processing files from 'DATASET/iqlinic/' to GCS prefix 'iqlinic'...\n",
      "2024-11-05 12:57:30,882 - INFO - Uploaded 'icliniq_chunk_90.csv' from 'DATASET/iqlinic/' to 'gs://raw_dataset_genai/iqlinic/icliniq_chunk_90.csv'\n",
      "2024-11-05 12:57:30,885 - INFO - Deleted local file 'DATASET/iqlinic/icliniq_chunk_90.csv' after upload.\n",
      "2024-11-05 12:57:30,885 - INFO - Processing files from 'DATASET/healthcaremagic/' to GCS prefix 'heathcaremagic'...\n",
      "2024-11-05 12:58:14,385 - INFO - Uploaded 'HealthCareMagic_chunk_90.json' from 'DATASET/healthcaremagic/' to 'gs://raw_dataset_genai/heathcaremagic/HealthCareMagic_chunk_90.json'\n",
      "2024-11-05 12:58:14,394 - INFO - Deleted local file 'DATASET/healthcaremagic/HealthCareMagic_chunk_90.json' after upload.\n",
      "2024-11-05 12:58:14,395 - INFO - Processing files from 'DATASET/mimic/' to GCS prefix 'mimic'...\n",
      "2024-11-05 12:59:43,864 - INFO - Uploaded 'mimic_chunk_90.json' from 'DATASET/mimic/' to 'gs://raw_dataset_genai/mimic/mimic_chunk_90.json'\n",
      "2024-11-05 12:59:43,879 - INFO - Deleted local file 'DATASET/mimic/mimic_chunk_90.json' after upload.\n",
      "2024-11-05 12:59:43,880 - INFO - Processing files from 'DATASET/misc/' to GCS prefix 'misc'...\n",
      "2024-11-05 12:59:43,881 - INFO - No files found in 'DATASET/misc/'. Skipping to next directory.\n",
      "2024-11-05 12:59:43,881 - INFO - Ingestion process completed.\n"
     ]
    }
   ],
   "source": [
    "# Run the ingestion function\n",
    "upload_files_to_gcs_multiple_sources(bucket_name, local_directories, blob_prefixes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
